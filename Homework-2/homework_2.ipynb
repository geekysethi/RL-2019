{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PA-2.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOGFlxCYp1Px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "from matplotlib import pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szRk__BSRYYN",
        "colab_type": "text"
      },
      "source": [
        "# Question 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5tNxHcop6Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GridWorldClass():\n",
        "\n",
        "\tdef __init__(self):\n",
        "\n",
        "\t\tself.worldSize = 5\n",
        "\t\tself.discountFactor = 0.9\n",
        "\t\tself.totalEpochs =1000\n",
        "\t\tself.error = 1e-9\n",
        "\t\tself.Aposition = [0,1]\n",
        "\t\tself.APrimePosition = [4,1]\n",
        "\t\tself.BPosition = [0,3]\n",
        "\t\tself.BPrimePosition = [2,3]\n",
        "\t\tself.right = [0,1]\n",
        "\t\tself.left = [0,-1]\n",
        "\t\tself.up = [-1,0]\n",
        "\t\tself.down = [1,0]\n",
        "\t\tself.actionList = [self.right,self.left,self.up,self.down]\n",
        "\n",
        "\tdef _stepFunction(self,action, currentState):\n",
        "\n",
        "\n",
        "\t\tif(currentState ==self.Aposition):\n",
        "\t\t\tnextState = self.APrimePosition\n",
        "\t\t\trewards = +10\n",
        "\t\t \n",
        "\t\telif(currentState ==self.BPosition):\n",
        "\t\t\tnextState = self.BPrimePosition\n",
        "\t\t\trewards = +5\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tnextState = [currentState[0]+action[0],currentState[1]+action[1]]\n",
        "\t\t\trewards = 0\n",
        "\n",
        "\t\t\tif(nextState[0]<0 or nextState[0]>=self.worldSize or nextState[1]<0 or nextState[1]>=self.worldSize):\n",
        "\t\t\t\trewards =-1\n",
        "\t\t\t\tnextState=currentState\n",
        "\t\t\t\t\n",
        "\n",
        "\t\treturn rewards, nextState\n",
        "\n",
        "\n",
        "\n",
        "\tdef simulateLinear(self):\n",
        "\t\tgrid = np.zeros ((self.worldSize,self.worldSize))\n",
        "\t\tprint(grid)\n",
        "\t\twhile(True):\n",
        "\t\t\tnewGrid = np.zeros ((self.worldSize,self.worldSize))\n",
        "\t\t\t# print(\"EPOCH: \"+str(currentEpoch))\n",
        "\t\t\tfor currentRow in range(self.worldSize):\n",
        "\t\t\t\tfor currentColumn in range(self.worldSize):\n",
        "\t\t\t\t\tfor currentAction in self.actionList:\n",
        "\t\t\t\t\t\tnewRewards,newState = self._stepFunction(currentAction,[currentRow,currentColumn])\n",
        "\t\t\t\t\t\tnewGrid[currentRow,currentColumn]+= 0.25 * (newRewards+self.discountFactor*grid[newState[0],newState[1]])\n",
        "\t\t\t\n",
        "\t\t\tif(np.sum(np.abs(newGrid-grid))<self.error):\n",
        "\t\t\t\tprint(np.round(grid,1))\n",
        "\t\t\t\tbreak\n",
        "\t\t\t\t\n",
        "\t\t\telse:\n",
        "\t\t\t\tgrid = newGrid\n",
        "\t\t \n",
        "\t\t\n",
        "\tdef simulateOptimal(self):\n",
        "\t\tgrid = np.zeros ((self.worldSize,self.worldSize))\n",
        "\t\tprint(grid)\n",
        "\t\twhile(True):\n",
        "\t\t\tnewGrid = np.zeros ((self.worldSize,self.worldSize))\n",
        "\t\t\t# print(\"EPOCH: \"+str(currentEpoch))\n",
        "\t\t\tfor currentRow in range(self.worldSize):\n",
        "\t\t\t\tfor currentColumn in range(self.worldSize):\n",
        "\t\t\t\t\ttemp=[]\n",
        "\t\t\t\t\tfor currentAction in self.actionList:\n",
        "\t\t\t\t\t\tnewRewards,newState = self._stepFunction(currentAction,[currentRow,currentColumn])\n",
        "\t\t\t\t\t\ttemp.append( newRewards+self.discountFactor*grid[newState[0],newState[1]])\n",
        "\t\t\t\t\tnewGrid[currentRow,currentColumn]=np.max(temp)\n",
        "\t\t\tif(np.sum(np.abs(newGrid-grid))<self.error):\n",
        "\t\t\t\tprint(np.round(grid,1))\n",
        "\t\t\t\tbreak\n",
        "\t\t\telse:\n",
        "\t\t\t\tgrid = newGrid\n",
        "\t\t \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONdi-BJ8p7fw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "4b8d0262-4449-4f9f-ebda-0bdfbe7593ce"
      },
      "source": [
        "gridWorldObject = GridWorldClass()\n",
        "gridWorldObject.simulateLinear()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "[[ 3.3  8.8  4.4  5.3  1.5]\n",
            " [ 1.5  3.   2.3  1.9  0.5]\n",
            " [ 0.1  0.7  0.7  0.4 -0.4]\n",
            " [-1.  -0.4 -0.4 -0.6 -1.2]\n",
            " [-1.9 -1.3 -1.2 -1.4 -2. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viJbl_i1RxFh",
        "colab_type": "text"
      },
      "source": [
        "# Question-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg-staBjqCfx",
        "colab_type": "code",
        "outputId": "d2aec403-24d5-4e1a-cf36-334db4d03287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "gridWorldObject.simulateOptimal()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "[[22.  24.4 22.  19.4 17.5]\n",
            " [19.8 22.  19.8 17.8 16. ]\n",
            " [17.8 19.8 17.8 16.  14.4]\n",
            " [16.  17.8 16.  14.4 13. ]\n",
            " [14.4 16.  14.4 13.  11.7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBdKB4qVqE8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "class GridWorldClass():\n",
        "\n",
        "\tdef __init__(self):\n",
        "\n",
        "\t\tself.worldSize = 4\n",
        "\t\tself.discountFactor = 0.9\n",
        "\t\t\n",
        "\t\t# self.error = 1e-4\n",
        "\t\tself.right = [0,1]\n",
        "\t\tself.left = [0,-1]\n",
        "\t\tself.up = [-1,0]\n",
        "\t\tself.down = [1,0]\n",
        "\t\tself.theta = 1e-4\n",
        "\n",
        "\t\tself.actionList = [self.left,self.up,self.right,self.down]\n",
        "\t\t\n",
        "\n",
        "\t\n",
        "\tdef _stepFunction(self,action, currentState):\n",
        "\t\tnextState = [currentState[0]+action[0],currentState[1]+action[1]]\n",
        "\n",
        "\n",
        "\t\tif (currentState[0]==0 and currentState[1]==0) or (currentState[0]==self.worldSize-1 and currentState[1]==self.worldSize-1):\t\t\t\n",
        "\t\t\treturn 0.0,currentState\n",
        "\n",
        "\n",
        "\t\tif(nextState[0]<0 or nextState[0]>=self.worldSize or nextState[1]<0 or nextState[1]>=self.worldSize):\n",
        "\t\t\tnextState=currentState\n",
        "\n",
        "\n",
        "\t\trewards =-1.0\n",
        "\t\treturn rewards, nextState\n",
        "\n",
        "\tdef policyEvaluation(self,policy):\n",
        "\t\t\n",
        "\t\tself.newStateValues =  np.zeros ((self.worldSize,self.worldSize))\n",
        "\n",
        "\t\twhile(True):\n",
        "\t\t\tdelta  = 0.0\n",
        "\n",
        "\n",
        "\t\t\toldStateValues = self.newStateValues\n",
        "\t\t\t\n",
        "\t\t\tfor currentRow in range(self.worldSize):\n",
        "\t\t\t\tfor currentColumn in range(self.worldSize):\n",
        "\t\t\t\t\tv=self.newStateValues[currentRow,currentColumn]\n",
        "\n",
        "\t\t\t\t\tcurrentAction = self.actionList[int(policy[currentRow,currentColumn])]\n",
        "\t\t\t\t\tnewRewards,newState = self._stepFunction(currentAction,[currentRow,currentColumn])\n",
        "\t\t\t\t\tself.newStateValues[currentRow,currentColumn] = newRewards+self.discountFactor*self.newStateValues[newState[0],newState[1]]\n",
        "\t\t\t\t\ttemp = np.abs(v-self.newStateValues[currentRow,currentColumn])\n",
        "\t\t\t\t\tdelta = max(delta,temp)\n",
        "\t\t\t\t\t# print(delta)\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\t\tif(delta<self.theta):\n",
        "\t\t\t\t# print(\"CONVERGE\")\n",
        "\t\t\t\t# print(np.round(self.newStateValues,1))\n",
        "\t\t\t\treturn self.newStateValues\n",
        "\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\n",
        "\tdef oneStepLookAheaFunction(self,currentRow,currentColumn,V):\n",
        "\t\t\n",
        "\t\tallValues=np.zeros(4)\n",
        "\t\tfor currentAction in range(4):\n",
        "\t\t\tnewRewards,newState = self._stepFunction(self.actionList[currentAction],[currentRow,currentColumn])\n",
        "\t\t\t\t\t\n",
        "\t\t\tallValues[currentAction]= newRewards+self.discountFactor*V[newState[0],newState[1]]\n",
        "\t\treturn np.argmax(allValues)\n",
        "\n",
        "\n",
        "\t\n",
        "\tdef policyImprovement(self):\n",
        "\n",
        "\t\tpolicy = np.zeros((4,4))\n",
        "\t\tactionIndex = [0,1,2,3]\n",
        "\n",
        "\t\tfor currentRow in range(self.worldSize):\n",
        "\t\t\tfor currentColumn in range(self.worldSize):\n",
        "\t\t\t\tpolicy[currentRow,currentColumn]=np.random.choice(actionIndex)\n",
        "\n",
        "\t\t\t\n",
        "\n",
        "\t\tprint(\"INTIAL POLICY:\\n\",policy)\n",
        "\t\tprint(\"*************************\")\n",
        "\t\t# V=self.policyEvaluation(policy)\n",
        "\t\t\t\n",
        "\t\tcount=0\n",
        "\t\twhile(True):\n",
        "\n",
        "\t\t\t# print(\"COUNT: \",count)\n",
        "\n",
        "\t\t\tcount+=1\n",
        "\t\t\tV=self.policyEvaluation(policy)\n",
        "\t\t\tpolicyStable =True\n",
        "\t\t\ttempCount=0\n",
        "\n",
        "\t\t\tfor currentRow in range(self.worldSize):\n",
        "\t\t\t\tfor currentColumn in range(self.worldSize):\n",
        "\n",
        "\t\t\t\t\tchosenAction =policy[currentRow,currentColumn]\n",
        "\t\t\t\t\tactionTaken = self.oneStepLookAheaFunction(currentRow,currentColumn,V)\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tpolicy[currentRow,currentColumn] = actionTaken \n",
        "\t\t\t\t\tif(chosenAction!=actionTaken):\n",
        "\t\t\t\t\t\ttempCount+=1\n",
        "\t\t\t\t\t\tpolicyStable=False\n",
        "\t\t\t\t\t\n",
        "\t\t\tif(policyStable):\n",
        "\t\t\t\tprint(\"***********FINAL POLICY***********\")\n",
        "\t\t\t\tprint(policy)\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\t\n",
        "\n",
        "\n",
        "\tdef valueIterations(self):\n",
        "\n",
        "\t\tself.newStateValues =  np.zeros ((self.worldSize,self.worldSize))\n",
        "\n",
        "\t\twhile(True):\n",
        "\t\t\tdelta  = 0.0\n",
        "\n",
        "\n",
        "\t\t\toldStateValues = self.newStateValues\n",
        "\t\t\t\n",
        "\t\t\tfor currentRow in range(self.worldSize):\n",
        "\t\t\t\tfor currentColumn in range(self.worldSize):\n",
        "\t\t\t\t\tv=self.newStateValues[currentRow,currentColumn]\n",
        "\t\t\t\t\tallValues=np.zeros(4)\n",
        "\t\t\t\t\tfor index,currentAction in enumerate(self.actionList):\n",
        " \n",
        "\n",
        "\t\t\t\t\t\tnewRewards,newState = self._stepFunction(currentAction,[currentRow,currentColumn])\n",
        "\t\t\t\t\t\tallValues[index] =  newRewards+self.discountFactor*self.newStateValues[newState[0],newState[1]]\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\tself.newStateValues[currentRow,currentColumn] = max(allValues)\n",
        "\t\t\t\t\ttemp = np.abs(v-self.newStateValues[currentRow,currentColumn])\n",
        "\t\t\t\t\tdelta = max(delta,temp)\n",
        "\t\t\t\t\n",
        "\t\t\t\n",
        "\t\t\tif(delta<self.theta):\n",
        "\t\t\t\t# print(\"CONVERGE\")\n",
        "\t\t\t\t# print(np.round(self.newStateValues,1))\n",
        "\t\t\t\tbreak\n",
        "\t\t\n",
        "\n",
        "\t\t\t\t\n",
        "\t\tpolicy = np.zeros((4,4))\n",
        "\t\t\t\n",
        "\n",
        "\t\tprint(\"INTIAL POLICY:\\n\",policy)\n",
        "\t\tprint(\"*************************\")\n",
        "\n",
        "\t\tfor currentRow in range(self.worldSize):\n",
        "\t\t\tfor currentColumn in range(self.worldSize):\n",
        "\t\t\t\tallValues=np.zeros(4)\n",
        "\t\t\t\tfor index,currentAction in enumerate(self.actionList):\n",
        "\n",
        "\t\t\t\t\tnewRewards,newState = self._stepFunction(currentAction,[currentRow,currentColumn])\n",
        "\t\t\t\t\tallValues[index] = newRewards+self.discountFactor*self.newStateValues[newState[0],newState[1]]\n",
        "\n",
        "\t\t\t\tpolicy[currentRow,currentColumn] = np.argmax(allValues)\n",
        "\t\t\n",
        "\t\t\n",
        "\t\tprint(policy)\n",
        "\t\t\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp8psrqkSCEK",
        "colab_type": "text"
      },
      "source": [
        "# Question 6 (a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2tRusk0R_C7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gridWorldObject = GridWorldClass()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkB3WggjSShU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "6c78be95-bf9d-4602-b827-7a7a1bdc70d3"
      },
      "source": [
        "gridWorldObject.policyImprovement()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INTIAL POLICY:\n",
            " [[3. 2. 3. 3.]\n",
            " [2. 2. 3. 2.]\n",
            " [0. 1. 2. 3.]\n",
            " [3. 0. 0. 0.]]\n",
            "*************************\n",
            "***********FINAL POLICY***********\n",
            "[[0. 0. 0. 0.]\n",
            " [1. 0. 0. 3.]\n",
            " [1. 0. 2. 3.]\n",
            " [1. 2. 2. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VITyzAocTAZM",
        "colab_type": "text"
      },
      "source": [
        "# Question-6 (b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlBHo278SUS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "2c2dc907-cebb-4eb3-f95d-27a7cd2ebd6f"
      },
      "source": [
        "gridWorldObject.valueIterations()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INTIAL POLICY:\n",
            " [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "*************************\n",
            "[[0. 0. 0. 0.]\n",
            " [1. 0. 0. 3.]\n",
            " [1. 0. 2. 3.]\n",
            " [1. 2. 2. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80MeqLPESWbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}